** List project plan for current task

** Modify openreview note fetcher to use '?after=' param
*** Fetch Service
- [-] Modify runRelayFetch logic to store and use last known noteId (most recently fetched).
  - [X] Implement a note iterator/generator with a limit count
  - [ ] Store note nums in db such that it is easy to tell if there are gaps in our fetched list
    - e.g., forward note cursor, reverse note cursor
    - this would mean including all nums, including those we can't use (marked as 'unusable' or something)
  - [ ] write logic as if fetching from beginning w/ sort num:asc was efficient
- [ ] Create parallel fetch loop to re-run all papers (if scraper logic changes, for example)
  - [ ] Use named cursors, stored in mongodb
- [ ] Create test plan
  - [ ] Run against live site and verify
  - [ ] Run against mocked api
    - [ ] Koa-based api mimicking openreview
  - [ ] Profile and report api fetch times
- [ ] Delete downloaded htmls/artifacts when done
- [ ] Delete /tmp files created by chrome
- [ ] Reap dead chrome instances


** Next Actions
- merge fetch service / shadow-db / mongo query api
- Create extracted-field collection, store abs/pdf attempts/status individually
- Create cursor collection, store rest params, name (uniq)
- Create mongo migration
- add 'number' field to note
