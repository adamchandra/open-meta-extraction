* Open Extraction Works
  A set of services to spider and extract metadata (abstracts, titles, authors, pdf links) from given URLs.

  Services may be run individually as command line applications, or as an integrated pipeline with a REST API

** Overview
This project provides a set of services to take the URL of a webpage for a
research paper, and extract metadata for that paper. Metadata includes the
abstract, title, authors, and a URL for a PDF version of the paper. Spidering is
done using an automated Chrome browser. Once a URL is loaded into the browser, a
series of extraction rules are applied. The extractor checks for commonly used
metadata schemas, including Highwire Press, Dublin Core, OpenGraph, along with
non-standard variations of the most common schemas, and a growing list of
journal and domain-specific methods of embedding metadata in the head or body of
a webpage. Once a suitable schema is identified, the metadata fields are saved
to a local file system, then returned to the caller. If changes are made to the
spidering and/or extractor, such that re-running the system produces different
results, that fact is returned along with the results.




** Services and Commandline Applications
*** Spidering
*** Field Extraction
*** Integrated spider/extractor
*** REST Server Frontend



** Production machine setup and deployment
Full system deployment uses Postgres and Redis. Simplified versions of the system may be run without
Postgres and/or Redis.

Running with Postgres enables better tracking and statistics regarding success rate for spidering and metadata gathering.

Running with Redis allows the services to be run in separate node processess, which may be monitored for resource usage,
script freezing and blocking, and enables batch processing of lists of URLs to be spidered and extracted, rather than
on-demand through one-at-a-time REST requests.
